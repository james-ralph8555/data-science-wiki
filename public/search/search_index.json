{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"wiki \u251c\u2500\u2500 AWS \u2502 \u251c\u2500\u2500 Athena \u2502 \u251c\u2500\u2500 Aurora \u2502 \u251c\u2500\u2500 Batch \u2502 \u251c\u2500\u2500 CloudTrail \u2502 \u251c\u2500\u2500 Cloudwatch \u2502 \u251c\u2500\u2500 Data Pipelines \u2502 \u251c\u2500\u2500 DynamoDB \u2502 \u251c\u2500\u2500 EC2 \u2502 \u251c\u2500\u2500 Elastic Container Registry \u2502 \u251c\u2500\u2500 Elastic MapReduce \u2502 \u251c\u2500\u2500 Glue Data Catalog \u2502 \u251c\u2500\u2500 Glue \u2502 \u251c\u2500\u2500 IAM \u2502 \u251c\u2500\u2500 Key Management Service \u2502 \u251c\u2500\u2500 Kinesis Analytics \u2502 \u251c\u2500\u2500 Kinesis Data Firehose \u2502 \u251c\u2500\u2500 Kinesis Data Streams \u2502 \u251c\u2500\u2500 Kinesis Video Streams \u2502 \u251c\u2500\u2500 Kinesis \u2502 \u251c\u2500\u2500 Lambda \u2502 \u251c\u2500\u2500 OpenSearch \u2502 \u251c\u2500\u2500 PrivateLink \u2502 \u251c\u2500\u2500 Quicksight \u2502 \u251c\u2500\u2500 Redshift \u2502 \u251c\u2500\u2500 Rekognition \u2502 \u251c\u2500\u2500 Relational Database Service \u2502 \u251c\u2500\u2500 S3 \u2502 \u251c\u2500\u2500 SageMaker \u2502 \u2502 \u251c\u2500\u2500 Apache Spark with SageMaker \u2502 \u2502 \u251c\u2500\u2500 Automatic Model Tuning \u2502 \u2502 \u251c\u2500\u2500 BlazingText \u2502 \u2502 \u251c\u2500\u2500 Data Wrangler \u2502 \u2502 \u251c\u2500\u2500 DeepAR \u2502 \u2502 \u251c\u2500\u2500 Factorization Machines \u2502 \u2502 \u251c\u2500\u2500 High Level Services \u2502 \u2502 \u2502 \u251c\u2500\u2500 CodeGuru \u2502 \u2502 \u2502 \u251c\u2500\u2500 Comprehend \u2502 \u2502 \u2502 \u251c\u2500\u2500 Connect \u2502 \u2502 \u2502 \u251c\u2500\u2500 DeepLens \u2502 \u2502 \u2502 \u251c\u2500\u2500 Forecast \u2502 \u2502 \u2502 \u251c\u2500\u2500 Fraud Detector \u2502 \u2502 \u2502 \u251c\u2500\u2500 Kendra \u2502 \u2502 \u2502 \u251c\u2500\u2500 Lex \u2502 \u2502 \u2502 \u251c\u2500\u2500 Lookout \u2502 \u2502 \u2502 \u251c\u2500\u2500 Monitron \u2502 \u2502 \u2502 \u251c\u2500\u2500 Neuron \u2502 \u2502 \u2502 \u251c\u2500\u2500 Panorama \u2502 \u2502 \u2502 \u251c\u2500\u2500 Personalize \u2502 \u2502 \u2502 \u251c\u2500\u2500 Polly \u2502 \u2502 \u2502 \u251c\u2500\u2500 Rekognition \u2502 \u2502 \u2502 \u251c\u2500\u2500 TexTract \u2502 \u2502 \u2502 \u251c\u2500\u2500 TorchServe \u2502 \u2502 \u2502 \u251c\u2500\u2500 Transcribe \u2502 \u2502 \u2502 \u2514\u2500\u2500 Translate \u2502 \u2502 \u251c\u2500\u2500 IP Insights \u2502 \u2502 \u251c\u2500\u2500 Image Classification \u2502 \u2502 \u251c\u2500\u2500 Implementation and MLOps \u2502 \u2502 \u251c\u2500\u2500 K-Means \u2502 \u2502 \u251c\u2500\u2500 K-Nearest-Neighbors \u2502 \u2502 \u251c\u2500\u2500 Latent Dirichlet Allocation \u2502 \u2502 \u251c\u2500\u2500 Linear Learner \u2502 \u2502 \u251c\u2500\u2500 Neural Topic Model \u2502 \u2502 \u251c\u2500\u2500 Object Detection \u2502 \u2502 \u251c\u2500\u2500 Object2Vec \u2502 \u2502 \u251c\u2500\u2500 Principal Component Analysis \u2502 \u2502 \u251c\u2500\u2500 Random Cut Forest \u2502 \u2502 \u251c\u2500\u2500 Reinforcement Learning \u2502 \u2502 \u251c\u2500\u2500 SageMaker Autopilot \u2502 \u2502 \u251c\u2500\u2500 SageMaker Canvas \u2502 \u2502 \u251c\u2500\u2500 SageMaker Clarify \u2502 \u2502 \u251c\u2500\u2500 SageMaker Debugger \u2502 \u2502 \u251c\u2500\u2500 SageMaker Edge Manager \u2502 \u2502 \u251c\u2500\u2500 SageMaker JumpStart \u2502 \u2502 \u251c\u2500\u2500 SageMaker Model Monitor \u2502 \u2502 \u251c\u2500\u2500 SageMaker Neo \u2502 \u2502 \u251c\u2500\u2500 SageMaker Notebooks \u2502 \u2502 \u251c\u2500\u2500 SageMaker Studio \u2502 \u2502 \u251c\u2500\u2500 SageMaker Training Compiler \u2502 \u2502 \u251c\u2500\u2500 SageMaker \u2502 \u2502 \u251c\u2500\u2500 Sagemaker Ground Truth \u2502 \u2502 \u251c\u2500\u2500 Semantic Segmentation \u2502 \u2502 \u251c\u2500\u2500 Seq2Seq \u2502 \u2502 \u2514\u2500\u2500 XGBoost \u2502 \u251c\u2500\u2500 Step Functions \u2502 \u2514\u2500\u2500 Virtual Private Cloud \u251c\u2500\u2500 Data Analysis and Preparation \u2502 \u251c\u2500\u2500 Binning \u2502 \u251c\u2500\u2500 Data Categories \u2502 \u251c\u2500\u2500 Distribution Types \u2502 \u251c\u2500\u2500 Imbalanced Data \u2502 \u251c\u2500\u2500 Missing Data Techniques \u2502 \u251c\u2500\u2500 Outliers In Dataset \u2502 \u251c\u2500\u2500 Probability Density Function \u2502 \u251c\u2500\u2500 SQL \u2502 \u251c\u2500\u2500 TF-IDF \u2502 \u251c\u2500\u2500 Time Series \u2502 \u251c\u2500\u2500 Transformations \u2502 \u2514\u2500\u2500 Visualization Types \u251c\u2500\u2500 Exam Prep Resources \u251c\u2500\u2500 Machine Learning \u2502 \u251c\u2500\u2500 Activation Functions \u2502 \u251c\u2500\u2500 Confusion Matrix \u2502 \u251c\u2500\u2500 Convolutional Neural Networks \u2502 \u251c\u2500\u2500 Debugging \u2502 \u251c\u2500\u2500 Ensemble Methods \u2502 \u251c\u2500\u2500 Neural Network Tuning \u2502 \u251c\u2500\u2500 Overfitting \u2502 \u251c\u2500\u2500 Recurrent Neural Networks \u2502 \u251c\u2500\u2500 Regularization \u2502 \u2514\u2500\u2500 SciKitLearn \u2514\u2500\u2500 index 5 directories, 107 files","title":"Home"},{"location":"Exam%20Prep%20Resources/","text":"Exam Prep Resources \u00b6 AWS Certified Machine Learning Specialty \u00b6 udemy: \"AWS Certified Machine Learning Specialty 2022 - Hands On!\" by Sundog Education. Comprehensive set of videos to teach you what you need for this exam. Do not buy their practice exam since you only get 1 for $15 and instead opt for one of the options below. Cost: $15 AWS Skill builder: login to skill builder and search for \"Exam Readiness: AWS Certified Machine Learning - Specialty\". The quiz/practice test in this module is worthwhile. Cost: Free TutorialsDojo: \"AWS Certified Machine Learning Specialty Practice Exams 2022\" by Jon-Bonso. 2 full length practice exams. Cost: ~$15","title":"Exam Prep Resources"},{"location":"Exam%20Prep%20Resources/#exam-prep-resources","text":"","title":"Exam Prep Resources"},{"location":"Exam%20Prep%20Resources/#aws-certified-machine-learning-specialty","text":"udemy: \"AWS Certified Machine Learning Specialty 2022 - Hands On!\" by Sundog Education. Comprehensive set of videos to teach you what you need for this exam. Do not buy their practice exam since you only get 1 for $15 and instead opt for one of the options below. Cost: $15 AWS Skill builder: login to skill builder and search for \"Exam Readiness: AWS Certified Machine Learning - Specialty\". The quiz/practice test in this module is worthwhile. Cost: Free TutorialsDojo: \"AWS Certified Machine Learning Specialty Practice Exams 2022\" by Jon-Bonso. 2 full length practice exams. Cost: ~$15","title":"AWS Certified Machine Learning Specialty"},{"location":"AWS/Athena/","tags":["Serverless"],"text":"For serverless SQL queries of S3 data Supports CSV, JSON, ORC, Parquet, and Avro Columnar formats such as ORC and Parquet deliver superior performance and cost savings Glue Data Catalog s can inform Athena Security managed via IAM policies Much of Athena's security is governed by S3 policies","title":"Athena"},{"location":"AWS/Aurora/","text":"","title":"Aurora"},{"location":"AWS/Batch/","text":"For running general batch jobs from Docker images","title":"Batch"},{"location":"AWS/CloudTrail/","text":"","title":"CloudTrail"},{"location":"AWS/Cloudwatch/","text":"","title":"Cloudwatch"},{"location":"AWS/Data%20Pipelines/","text":"Orchestration service to provision EC2 instances to move data around from above data stores","title":"Data Pipelines"},{"location":"AWS/DynamoDB/","text":"NoSQL (not only SQL), serverless, provisioned read/write capacity, general purpose database","title":"DynamoDB"},{"location":"AWS/EC2/","text":"","title":"EC2"},{"location":"AWS/Elastic%20Container%20Registry/","text":"","title":"Elastic Container Registry"},{"location":"AWS/Elastic%20MapReduce/","tags":["ETL"],"text":"Includes Spark (like Glue ), Presto (like Athena ), Flink, Hive (data warehouse) EMR Clusters have a main node on a single EC2 instance which manages the cluster Core nodes contain the Hadoop distributed file system (HDFS) and can run tasks Task nodes do not store data and can be added/removed from cluster as needed HDFS is emphemeral and is lost when the cluster is removed EMRFS works like HDFS but contains S3 data Apache Spark MLLib can perform machine learning tasks EMR can be managed from an EMR notebook (Jupyter notebook) EMR security is managed with IAM policies and IAM roles","title":"Elastic MapReduce"},{"location":"AWS/Glue%20Data%20Catalog/","text":"","title":"Glue Data Catalog"},{"location":"AWS/Glue/","tags":["ETL","Serverless"],"text":"For transforming, enriching, or cleaning data before analysis Can store data back in S3 , Redshift , RDS , or the Glue Data Catalog Fully managed Can be scheduled or run automatically Transformation type examples: DropFields, DropNullFields - remove (null fields) Filter - filter records Join - add other data to dataset Map - add/delete fields, perform extermal lookups Machine learning transform - FindMatchesML - for (fuzzy) data deduplication All Apache Spark transforms can be used Cannot write to recordIO-protobuf","title":"Glue"},{"location":"AWS/IAM/","text":"","title":"IAM"},{"location":"AWS/Key%20Management%20Service/","text":"","title":"Key Management Service"},{"location":"AWS/Kinesis%20Analytics/","text":"For real time analytics/streaming ETL on Kinesis using SQL or Apache Flink Pay per resources consumed; serverless and auto-scaling Security controlled via IAM Can perform streaming ML: RANDOM_CUT_FOREST: generic anomaly detection algorithm, trains off random data, performs Random Cut Forest HOTSPOTS: finds dense regions/clusters in data","title":"Kinesis Analytics"},{"location":"AWS/Kinesis%20Data%20Firehose/","tags":["Serverless"],"text":"Not real-time; minimum 60 seconds latency Fully managed/Auto scaling For loading streams into S3 , Redshift (COPY through S3), ElasticSearch , and 3rd party partners e.g. Splunk Supports many formats and data conversions from CSV/JSON to Parquet/ORC when writing to S3 Supports compression when writing to S3 Allows arbitrary data transforms with Lambda Can read from applications, Cloudwatch , or Kinesis Data Streams Pay per data sent through data firehose","title":"Kinesis Data Firehose"},{"location":"AWS/Kinesis%20Data%20Streams/","text":"Low latency streaming ingest at scale Streams are partitioned by shards Shards can be provisioned manually or with an API call Each provisioned shard gets 1MB/s in and 2 MB/s out On-demand mode can be used to have capacity automatically managed by AWS based on throughput peak in last 30 days Data retention is 24 hours by default, allows for up to 365 days Can have multiple consumers for same stream Each record inserted into stream can be up to 1MB Use Kinesis Client Library (KCL) to handle distributed consumption and computation on kinesis data streams Use Kinesis Producer Library (KPL) to help with writing to a kinesis data stream","title":"Kinesis Data Streams"},{"location":"AWS/Kinesis%20Video%20Streams/","text":"Takes in video stream Can send data to services like Rekognition or Sagemaker ML models Stores data from 1 hour to 10 years","title":"Kinesis Video Streams"},{"location":"AWS/Kinesis/","text":"Kinesis is a managed streaming service to Apache Kafka Manages real time data from logs, IoT metrics, or clickstream data Use Kinesis Data Streams for realtime streaming Use Kinesis Data Firehose for low-latency (but not real time) fully-managed ingest Use Kinesis Analytics for real-time analytics/streaming ETL on kinesis Use Kinesis Video Streams for video streams","title":"Kinesis"},{"location":"AWS/Lambda/","text":"","title":"Lambda"},{"location":"AWS/OpenSearch/","text":"","title":"OpenSearch"},{"location":"AWS/PrivateLink/","text":"","title":"PrivateLink"},{"location":"AWS/Quicksight/","tags":["Serverless","AnomalyDetection","Forecasting"],"text":"Business Analytics tool Accepts many sources - Redshift , Athena , Aurora , RDS , Uploaded Files, S3 Gives easy machine learning insights using Random Cut Forest for anomaly detection Can forecast Time Series Can give plain text explainations of data using auto-narratives","title":"Quicksight"},{"location":"AWS/Redshift/","text":"Data warehousing and SQL analytics platform Data is loaded from S3 , usually with COPY command Columnar datastore therefore efficient for queries Redshift spectrum can be used to directly query S3 data","title":"Redshift"},{"location":"AWS/Rekognition/","text":"","title":"Rekognition"},{"location":"AWS/Relational%20Database%20Service/","text":"SQL, relational, row based data stores","title":"Relational Database Service"},{"location":"AWS/S3/","text":"Use Cases \u00b6 Data lake Back up and restore data Achive data at the lowest cost Properties \u00b6 Object Storage Buckets must have a globally unique name Max object size of 5TB 99.99999999999% (11 9's) object durability Partitions should be setup to optimize queries Data that is queried predominately by date would be s3://bucket/dataset/year/month/day/product/data.csv Data that is queried predominately by product would be s3://bucket/dataset/product/year/month/day/data.csv S3 storage classes comparison: \u00b6 Standard 99.99% availability - objects may not be available 53 minutes per year Good for general purpose storage e.g. big data analytics Infrequent Access For data that requires rapid access but is less frequently accessed e.g. disaster recovery and backups 99.9% availability Need to wait 30 days to transition S3 standard data to IA One Zone Infrequent Access Same as infrequent access but data is only replicated in a single availibility zone 99.5% availability Useful for secondary backup copies of data or data that can be recreated All tiers below and including this one incur retrieval fees Glacier Instant Retrieval For long term data storage that needs to be accessed instantly Minimum storage duration of 90 days Glacier Flexible Retrieval For long term storage where data does not need to be accessed instantly Offers expedited (1-5 mins), standard (3-5 hours) and bulk (5-12 hours) access times Minimum storage duration of 90 days Glacier Deep Archive Offers standard (12 hours) and bulk (48 hours) retrieval options Minimum storage duration of 180 days S3 Intelligent tiering to auto manage object storage class based on access patterns S3 lifecycle classes can be used to define rules for a bucket, a folder (prefix) in a bucket, or object tag: Transition rules can be setup to move objects to a certain storage class after a specified time Expiration rules can be setup to delete files after a specified time or delete old versions of files S3 Security \u00b6 S3 Encryption: SSE-S3: server size encryption (S3) fully managed by AWS SSE- KMS : uses AWS Key Management server to manage encryption keys; allows audit trail for KMS key usage SSE-C: user manages own encrypton keys Client side encryption: files encrypted by user before upload S3 access can be configured by user with IAM policies to give certain action access by user (e.g. read or write access) S3 access can be set up based on resource using bucket policies to set bucket-wide policies. With these, one can force encryption, allow cross account access, and grant public access to an entire bucket. S3 bucket policies are configured in JSON S3 resource based policies can also be configured with an Object Access Control List (ACL) to set permissions by object","title":"Use Cases"},{"location":"AWS/S3/#use-cases","text":"Data lake Back up and restore data Achive data at the lowest cost","title":"Use Cases"},{"location":"AWS/S3/#properties","text":"Object Storage Buckets must have a globally unique name Max object size of 5TB 99.99999999999% (11 9's) object durability Partitions should be setup to optimize queries Data that is queried predominately by date would be s3://bucket/dataset/year/month/day/product/data.csv Data that is queried predominately by product would be s3://bucket/dataset/product/year/month/day/data.csv","title":"Properties"},{"location":"AWS/S3/#s3-storage-classes-comparison","text":"Standard 99.99% availability - objects may not be available 53 minutes per year Good for general purpose storage e.g. big data analytics Infrequent Access For data that requires rapid access but is less frequently accessed e.g. disaster recovery and backups 99.9% availability Need to wait 30 days to transition S3 standard data to IA One Zone Infrequent Access Same as infrequent access but data is only replicated in a single availibility zone 99.5% availability Useful for secondary backup copies of data or data that can be recreated All tiers below and including this one incur retrieval fees Glacier Instant Retrieval For long term data storage that needs to be accessed instantly Minimum storage duration of 90 days Glacier Flexible Retrieval For long term storage where data does not need to be accessed instantly Offers expedited (1-5 mins), standard (3-5 hours) and bulk (5-12 hours) access times Minimum storage duration of 90 days Glacier Deep Archive Offers standard (12 hours) and bulk (48 hours) retrieval options Minimum storage duration of 180 days S3 Intelligent tiering to auto manage object storage class based on access patterns S3 lifecycle classes can be used to define rules for a bucket, a folder (prefix) in a bucket, or object tag: Transition rules can be setup to move objects to a certain storage class after a specified time Expiration rules can be setup to delete files after a specified time or delete old versions of files","title":"S3 storage classes comparison:"},{"location":"AWS/S3/#s3-security","text":"S3 Encryption: SSE-S3: server size encryption (S3) fully managed by AWS SSE- KMS : uses AWS Key Management server to manage encryption keys; allows audit trail for KMS key usage SSE-C: user manages own encrypton keys Client side encryption: files encrypted by user before upload S3 access can be configured by user with IAM policies to give certain action access by user (e.g. read or write access) S3 access can be set up based on resource using bucket policies to set bucket-wide policies. With these, one can force encryption, allow cross account access, and grant public access to an entire bucket. S3 bucket policies are configured in JSON S3 resource based policies can also be configured with an Object Access Control List (ACL) to set permissions by object","title":"S3 Security"},{"location":"AWS/Step%20Functions/","text":"For defining workflows from other AWS components e.g. data engineering processes Allows for auditing workflows Gives error handling and retry abilities to data engineering workflows Allows for implementing conditionals based on outcome of jobs in workflow","title":"Step Functions"},{"location":"AWS/Virtual%20Private%20Cloud/","text":"aliases: [VPC] \u00b6","title":"Virtual Private Cloud"},{"location":"AWS/Virtual%20Private%20Cloud/#aliases-vpc","text":"","title":"aliases: [VPC]"},{"location":"AWS/SageMaker/Apache%20Spark%20with%20SageMaker/","text":"AWS Provides a SageMaker-Spark library to use SageMaker within Apache Spark Data preprocessing can be done in Spark, and then a SageMakerEstimator object can be used to generate a SageMakerModel to perform ML using SageMaker's algorithms in Spark","title":"Apache Spark with SageMaker"},{"location":"AWS/SageMaker/Automatic%20Model%20Tuning/","text":"Automates finding optimal hyperparameters ( Neural Network Tuning ) User defines important hyperparameters and ranges of those to explore Automatic model tuning learns as it goes (via Bayesian optimizer) so that every combination of hyperparameters is not tested Best Practices: Don't optimize too many hyperparameters to avoid dimensionality blowup Limit ranges as much as possible Use logarithmic instead of linear scales on hyperparameter ranges when appropriate Run jobs serially to maximize learning from past tunings","title":"Automatic Model Tuning"},{"location":"AWS/SageMaker/BlazingText/","tags":["NLP"],"text":"Can do text classification i.e. predict labels for a sentence Intended for use only with sentences, not whole documents Can also create vector representations of words (Word2vec) to feed into other NLP algorithms For text classification, input is fed in with one sentence per line For text classification, first item in each sentence needs to be __label__ followed by the label e.g. __label__4 linux ready for prime time , intel says . For text classification, all words and punctuation should be seperated with a space Word2vec accepts a text file with one training sentence per line Word2vec has multiple modes: Cbow (continuous bag of words) where word order doesn't matter Skip-gram where order does matter Batch skip-gram which is like skip-gram but can be distributed Word2vec hyperparams: mode (cbow, skipgram, batch_skipgram) learning_rate window_size vector_dim Text classification hyperparams: Epochs learning_rate word_ngrams vector_dim Training in cbow or skipgram mode, a single gpu instance will work Batch skipgram uses single or multiple CPU instances Text classification uses CPU nodes or a GPU node if the training data is over 2GB","title":"BlazingText"},{"location":"AWS/SageMaker/Data%20Wrangler/","tags":["ETL","Serverless"],"text":"Import/transform/analyze/export data within SageMaker Studio","title":"Data Wrangler"},{"location":"AWS/SageMaker/DeepAR/","tags":["Forecasting"],"text":"For forecasting 1D time series data Can take multiple related Time Series as input Detects frequencies and seasonality Takes JSON lines or Parquet format Input can contain dynamic features such as events or categorical features The entire dataset should be used as the test set, and the most recent values should be withheld from the training data Hyperparameters Context_length - how many time points the model sees before making predictions Epochs mini_batch_size learning_rate num_cells Can train on CPU or GPU, with single or multi machine Trains well on CPU for smaller models Inference is CPU-only","title":"DeepAR"},{"location":"AWS/SageMaker/Factorization%20Machines/","tags":["Classification"],"text":"For classification or regression of sparse data with sparse data Works well for reccomender systems since that data is inherently sparse since users interact with only a small subset of products Supervised algorithm, limited to pair-wise interactions e.g. User->item Takes recordIO-protobuf format in Float32 Works via finding factors to predict a classification or value given a matrix representing pairs (e.g. user->item) Hyperparameters: Initialization methods for bias, factors, and linear terms Uses CPU or GPU, but CPU recomended since GPUs are ill-suited for sparse data","title":"Factorization Machines"},{"location":"AWS/SageMaker/IP%20Insights/","tags":["AnomalyDetection"],"text":"Finds suspicious logins by IP address in web logs Input data is usernames or account IDs paired with an IP address paired with an IP address Works by using a neural network to learn latent vector representations of entities and IP addresses Automatically generates anomalous samples during training by randomly pairing usernames or account IDs with different IP addresses Hyperparameters: Num_entitiy_vectors - controls hash size. Values too large slow down algorithm, but values too small can cause hash collision issues. Reccomended to set to 2x the number of unique usernames/account IDS Vector_dim: size of embedding vectors - too large can result in overfitting Standard neural net hyperparameters GPU reccomended for training since it is neural network based, but CPUs can be used too","title":"IP Insights"},{"location":"AWS/SageMaker/Image%20Classification/","tags":["ComputerVision"],"text":"Assigns labels to an image Takes RecordIO format (not protobuf), or jpg or png with a .lst annotation file Accepts augmented manifest image format, and using this format allows pipe mode Uses a ResNet CNN , can train from scratch or do transfer learning from imageNet Hyperparameters: the usual Uses GPU for training, multi-GPU and multi-machine allowed","title":"Image Classification"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/","text":"Sagemaker/Docker integration \u00b6 All sagemaker models are hosted in docker containers registed with the Elastic Container Registry (ECR) Allows any script or algorithm within SageMaker as long as the docker container follows a certain file structure Containres must be able to respond to both /invocations and /ping on port 8080, and accept socket connection requests within 250ms The file structure for a training container: The file structure for a deployment container: Docker image must contain: nginx.conf - config for nginx web gateway/frontend predictor.py - config for flask webserver to serve model and make predictions serve/ directory - deployment model directory, launches gunicorn server to launch multiple flask webservers train/ directory - used for training wsgi.py - wrapper for invoking flask application Can have different training/inference images or they can be combined into the above image Environment variables in the dockerfile can define where sagemaker can find programs in the container, checkpoint directories, etc e.g. include \"ENV SAGEMAKER_PROGRAM train.py\" in the docker file to set /opt/ml/code/train.py as the training entrypoint - SageMaker does not do this automatically Hyperparameters can be set via environment variables for hyperparameter tuning Multiple variants of a model can be deployed at once Variant weights determine how often each model is used Useful for A/B testing where the existing model gets the most traffic and the new model gets a higher share of users as its effectiveness is proven Edge Deployment: Neo and IoT Greengrass \u00b6 Neo provides a way to deploy trained models to edge devices e.g. NVIDIA Jetson Code is compiled and optimized for chosen device Good for very-low latency applications e.g. embedded applications or applications where internet is not available IoT greengrass delivers Neo compiled modeleds to an actual edge device Security \u00b6 General best practices: Use IAM service to set up accounts by the principal of least access Use MFA Use encrption in transit (SSL/TLS) Use Cloudtrail to log API and user activity Use encryption at rest AWS Key Management Service ( KMS ) is accepted by notebooks and all SageMaker jobs to encrypt data at rest S3 buckets can be encrypted for training data and hosting models, potentially with KMS All sagemaker traffic supports SSL/TLS IAM roles are assigned to SageMaker to give it permissions to access resources When doing distributed training, communications between nodes can be encrypted but this will increase training time Sagemaker does not support resource based policies, but supports authorization based on resource tags Granular IAM Permissions: CreateTrainingJob CreateModel CreateEndpointConfig CreateTransformJob CreateHyperParameterTuningJob CreateNotebookInstance UpdateNotebookInstance Predefined policies: AmazonSageMakerReadOnly AmazonSageMakerFullAccess AdministratorAccess DataScientist Cloudwatch can log, monitor, and alarm endpoints and their latency/health Cloudwatch can monitor number of human workers on Ground Truth Cloudwatch metrics are available at a 1-minute frequency CloudTrail records actions of users, roles, and services within SageMaker CloudTrail does not monitor calls to InvokeEndpoint For further reading, reference the \"Infrastructure Security\" in the SageMaker developer guide Virtual Private Cloud (VPC) \u00b6 All training jobs run in a Virtual Private Cloud Private VPCs can be setup for a training job To connect a private VPC to S3 data, S3 VPC endpoints must be setup These VPC endpoints are secured with their own policies and S3 bucket policies SageMaker Notebooks are internet-enabled by default If internet access is disabled for notebook s, and interface endpoint ( PrivateLink ) or NAT Gateway and configure this to allow outbound connections Training and inference containers are also internet-enabled - these can be isolated from the network but this prevents S3 access Managing Resources \u00b6 In general, use GPUs for deep learning training Inference is less demanding an can be run on CPU instances Managed spot training can use EC2 spot instances for training for large cost savings Checkpoints should be used so training can be resumed if/when spot instances are interrupted Automatic scaling can dynamically adjust resources used according to current load Elastic Inference (EI) \u00b6 Used to accelerate deep learning inference EI accelerator is coupled with CPU instance This gives benefits of a GPU but for cheaper Can be applied to notebooks, Tensorflow/MXNet containers Works with Image Classification and ObjectDetection built-in algorithms Availability Zones \u00b6 When using multiple instances in production, sagemaker automatically attempts to distribute these accross availability zones VPC's should have a subnet for each availibility zone Serverless Inference \u00b6 Fully managed inference service Container, memory requirement, and concurrency are specified Capacity is automatically provisioned and scaled Can be monitored from cloud watch Inference reccomender \u00b6 Reccomends best instance type & config for models Benchmarks different instance types on a given model Can perform custom load tests to guarantee performance Inference Pipelines \u00b6 Can deploy inference via a sequence of 2-15 containers Any combination of pre-trained built in algorithms or custom containerized algorithms work Can handle real-time inference and for batch transforms SageMaker Notebooks \u00b6 SageMaker Notebooks - Only files and data saved within the /home/ec2-user/SageMaker folder persist between notebook instance sessions. Files and data that are saved outside this directory are overwritten when the notebook instance stops and restarts. - Notebook access any s3 bucket with \"sagemaker\" in its name with default IAM permissions Distributed training \u00b6 Two modes - fully replicated and sharded by S3 - set in \"S3DataSource/S3DataDistributionType\" in sagemaker config Fully replicated - full dataset given to all nodes Increased model stability Scaling resource usage Good for smaller data amounts Sharded by S3 - data chunked into smaller keys by S3 key Good for larger data amounts Faster training Each model exposed to only a subset of data Use Horovod to distibute training","title":"Implementation and MLOps"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#sagemakerdocker-integration","text":"All sagemaker models are hosted in docker containers registed with the Elastic Container Registry (ECR) Allows any script or algorithm within SageMaker as long as the docker container follows a certain file structure Containres must be able to respond to both /invocations and /ping on port 8080, and accept socket connection requests within 250ms The file structure for a training container: The file structure for a deployment container: Docker image must contain: nginx.conf - config for nginx web gateway/frontend predictor.py - config for flask webserver to serve model and make predictions serve/ directory - deployment model directory, launches gunicorn server to launch multiple flask webservers train/ directory - used for training wsgi.py - wrapper for invoking flask application Can have different training/inference images or they can be combined into the above image Environment variables in the dockerfile can define where sagemaker can find programs in the container, checkpoint directories, etc e.g. include \"ENV SAGEMAKER_PROGRAM train.py\" in the docker file to set /opt/ml/code/train.py as the training entrypoint - SageMaker does not do this automatically Hyperparameters can be set via environment variables for hyperparameter tuning Multiple variants of a model can be deployed at once Variant weights determine how often each model is used Useful for A/B testing where the existing model gets the most traffic and the new model gets a higher share of users as its effectiveness is proven","title":"Sagemaker/Docker integration"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#edge-deployment-neo-and-iot-greengrass","text":"Neo provides a way to deploy trained models to edge devices e.g. NVIDIA Jetson Code is compiled and optimized for chosen device Good for very-low latency applications e.g. embedded applications or applications where internet is not available IoT greengrass delivers Neo compiled modeleds to an actual edge device","title":"Edge Deployment: Neo and IoT Greengrass"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#security","text":"General best practices: Use IAM service to set up accounts by the principal of least access Use MFA Use encrption in transit (SSL/TLS) Use Cloudtrail to log API and user activity Use encryption at rest AWS Key Management Service ( KMS ) is accepted by notebooks and all SageMaker jobs to encrypt data at rest S3 buckets can be encrypted for training data and hosting models, potentially with KMS All sagemaker traffic supports SSL/TLS IAM roles are assigned to SageMaker to give it permissions to access resources When doing distributed training, communications between nodes can be encrypted but this will increase training time Sagemaker does not support resource based policies, but supports authorization based on resource tags Granular IAM Permissions: CreateTrainingJob CreateModel CreateEndpointConfig CreateTransformJob CreateHyperParameterTuningJob CreateNotebookInstance UpdateNotebookInstance Predefined policies: AmazonSageMakerReadOnly AmazonSageMakerFullAccess AdministratorAccess DataScientist Cloudwatch can log, monitor, and alarm endpoints and their latency/health Cloudwatch can monitor number of human workers on Ground Truth Cloudwatch metrics are available at a 1-minute frequency CloudTrail records actions of users, roles, and services within SageMaker CloudTrail does not monitor calls to InvokeEndpoint For further reading, reference the \"Infrastructure Security\" in the SageMaker developer guide","title":"Security"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#virtual-private-cloud-vpc","text":"All training jobs run in a Virtual Private Cloud Private VPCs can be setup for a training job To connect a private VPC to S3 data, S3 VPC endpoints must be setup These VPC endpoints are secured with their own policies and S3 bucket policies SageMaker Notebooks are internet-enabled by default If internet access is disabled for notebook s, and interface endpoint ( PrivateLink ) or NAT Gateway and configure this to allow outbound connections Training and inference containers are also internet-enabled - these can be isolated from the network but this prevents S3 access","title":"Virtual Private Cloud (VPC)"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#managing-resources","text":"In general, use GPUs for deep learning training Inference is less demanding an can be run on CPU instances Managed spot training can use EC2 spot instances for training for large cost savings Checkpoints should be used so training can be resumed if/when spot instances are interrupted Automatic scaling can dynamically adjust resources used according to current load","title":"Managing Resources"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#elastic-inference-ei","text":"Used to accelerate deep learning inference EI accelerator is coupled with CPU instance This gives benefits of a GPU but for cheaper Can be applied to notebooks, Tensorflow/MXNet containers Works with Image Classification and ObjectDetection built-in algorithms","title":"Elastic Inference (EI)"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#availability-zones","text":"When using multiple instances in production, sagemaker automatically attempts to distribute these accross availability zones VPC's should have a subnet for each availibility zone","title":"Availability Zones"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#serverless-inference","text":"Fully managed inference service Container, memory requirement, and concurrency are specified Capacity is automatically provisioned and scaled Can be monitored from cloud watch","title":"Serverless Inference"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#inference-reccomender","text":"Reccomends best instance type & config for models Benchmarks different instance types on a given model Can perform custom load tests to guarantee performance","title":"Inference reccomender"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#inference-pipelines","text":"Can deploy inference via a sequence of 2-15 containers Any combination of pre-trained built in algorithms or custom containerized algorithms work Can handle real-time inference and for batch transforms","title":"Inference Pipelines"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#sagemaker-notebooks","text":"SageMaker Notebooks - Only files and data saved within the /home/ec2-user/SageMaker folder persist between notebook instance sessions. Files and data that are saved outside this directory are overwritten when the notebook instance stops and restarts. - Notebook access any s3 bucket with \"sagemaker\" in its name with default IAM permissions","title":"SageMaker Notebooks"},{"location":"AWS/SageMaker/Implementation%20and%20MLOps/#distributed-training","text":"Two modes - fully replicated and sharded by S3 - set in \"S3DataSource/S3DataDistributionType\" in sagemaker config Fully replicated - full dataset given to all nodes Increased model stability Scaling resource usage Good for smaller data amounts Sharded by S3 - data chunked into smaller keys by S3 key Good for larger data amounts Faster training Each model exposed to only a subset of data Use Horovod to distibute training","title":"Distributed training"},{"location":"AWS/SageMaker/K-Means/","tags":["Clustering"],"text":"Unsupervised clustering algorithm - divides data into K similar groups by some distance metric e.g. Euclidean distance Takes recordIO-protobuf or CSV input format, with file or pipe mode for either Each observation is mapped to a n-dimensional space Hyperparameters: K - number of clusters - can use \"elbow method\" on a plot of total within-cluster sum of squares (WSS) as a function of K to find a good number Mini_batch_size Extra_center_factor - adds extra clusters while training which are reduced down to K clusters Init_method - random or Kmeans++ - Kmeans++ starts clusters far apart to avoid initializing them too close together CPU or GPU for training, but CPU is reccomended","title":"K Means"},{"location":"AWS/SageMaker/K-Nearest-Neighbors/","tags":["Classification","Regression","Clustering"],"text":"Simple classification or regression algorithm Classification works by finding the K closest points and returning the most frequent label Regression works by finding the K closest points and averaging their values Input format is recordIO-protobuf or CSV training, with first column as the label Uses file or pipe mode for both Sagemaker samples the data and performs dimensionality reduction to optimize algorithm Hyperparameters: K - number of nearest neighbors to look at Sample_size Trains on CPU or GPU instances","title":"K Nearest Neighbors"},{"location":"AWS/SageMaker/Latent%20Dirichlet%20Allocation/","tags":["NLP","TopicModelling"],"text":"Unsupervised topic modeling algorithm, not based on deep learning Works on other type of data other than documents Input format is recordIO-protobuf or CSV Each document has counts for every word in the vocabulary (in CSV format) Pipe mode only supported for recordIO Like Neural Topic Model , the number of topics is specified Hyperparameters: Num_topics Alpha0 - initial guess for concentration parameter - smaller values produce sparser topic mixtures while larger values produce more uniform mixtures Single instance CPU only for training","title":"Latent Dirichlet Allocation"},{"location":"AWS/SageMaker/Linear%20Learner/","tags":["Classification","Regression","DisitributedTraining"],"text":"Linear regression model that trains like a ML model Handles regression and binary/multi-class classification Takes in RecordIO-wrapped protobuf with float32 data or CSV data First column assumed to be the label column when using CSV S3 file and pipe (streaming) mode both supported Training data must be normalized and shuffled Uses an optimization algorithm like SGD Hyperparameters: Balance_multiclass_weights - gives each class equal importance in loss functions Learning_rate mini_batch_size L1 Regularization L2 regularization weight decay Trains on single or multi-machine GPU (multiple GPUs on one machine does not help)","title":"Linear Learner"},{"location":"AWS/SageMaker/Neural%20Topic%20Model/","tags":["NLP","TopicModelling"],"text":"Topic modeling assigns a general label to a documents Topics are not human-readable or meaningful on their own Useful for classification or summarization of documents Unsupervised algorithm - uses neural variational inference Takes recordIO-protobuf or CSV data via file or pipe mode, with each word tokenized into integers and a vocabulary file Hyperparameters: Num_topics - number of topics to classify documents into General ML Hyperparameters GPU or CPU used for training","title":"Neural Topic Model"},{"location":"AWS/SageMaker/Object%20Detection/","text":"Identifies all objects in image, with bounding boxes and confidence scores Can train from scratch, use a pre-trained model, or use transfer learning from imageNet Input data can be RecordIO or images (jpg or png) with JSON annotation data Uses a CNN with the Single Shot multibox detector (SSD) algorithm CNN can either be VGG-16 or ResNet-50 Does data augmentation (flip, rescale, jitter) automatically Hyperparameters: the usual deep learning hyperparameters Uses GPU for training, multi-GPU and multi-machine allowed","title":"Object Detection"},{"location":"AWS/SageMaker/Object2Vec/","tags":["DimensionalityReduction"],"text":"Like word2vec form blazing text, but for arbitrary objects e.g. sentences or documents Can be used to compute nearest neighbors of objects, to visualize clusters, predict genres, or for reccomendations Input data must be tokenized into integers Training data consists of pairs of tokens and/or sequences of tokens e.g. pairing a sentence token with another sentence token or pairing a user token with an item token Pairs of tokens are fed into their own encoders, which are then input into a comparitor whose input is sent to a feed-forward neural network Encoder can be an averaged-pooled embedder, a CNN , or bidirectional RNN : each has different strengths depending on the input data Hyperparameters: Usual deep learning params: dropout, learning_rate, etc. Encoder networks: CNN, BiLSTM, pooled embedding Can only train on a single CPU or GPU instance, multi-GPU is allowed","title":"Object2Vec"},{"location":"AWS/SageMaker/Principal%20Component%20Analysis/","tags":["DimensionalityReduction"],"text":"Unsupervised dimensionality reduction - projects features into lower dimensional space with minimal information (variance) loss Components in lower dimensional space are meaningless to humans but retain information so they are useful for statistical algorithms Input is recordIO-protobuf or CSV in pipe or filemode Works by computing covariance matrix and reducing it with singular value decomposition Can work in regular mode which uses all data, or randomized mode for large datasets which approximates data Hyperparameters: Algorithm_mode - regular or randomized Subtract_mean - unbiases data Can use GPU or CPU","title":"Principal Component Analysis"},{"location":"AWS/SageMaker/Random%20Cut%20Forest/","tags":["AnomalyDetection"],"text":"Unsupervised anomaly detection algorithm Detects changes in complexity of a random forest with each datapoint: larger changes in complexity mean a datapoint is more anomalous Input format can be RecordIO-protobuf or CSV, in file or pipe mode Can work on streaming data via Kinesis analytics Hyperparameters: Num_trees - increasing this reduces noise Num_samples_per_tree - should be chosen such that 1/Num_samples_per_tree equals the ratio of anomalous to normal data CPU only","title":"Random Cut Forest"},{"location":"AWS/SageMaker/Reinforcement%20Learning/","text":"Reinforcement uses an agent (e.g. the Pac-Man avatar) in a space (the Pac-Man game) with a reward system (e.g. positive rewards for getting power pills, negative rewards for hitting a ghost or wall) Agent learns best action to take for a given space Practical applications include supply chain management, HVAC systems, industrial robotics For further information: read about Q-Learning and deep Q-learning Training done via deep learning Sagemaker assists via distributing training and rolling out environments","title":"Reinforcement Learning"},{"location":"AWS/SageMaker/SageMaker%20Autopilot/","text":"AutoML service Selects algorithm, preprocesses data, tunes model, and handles infrastructure Generates Jupyter notebook to show what is happening User selects target feature for training Shows leaderboard of model performance Data must be tabular CSV Works with binary classification, multiclass classification, and regression Works with Linear Learner , XGBoost , and deep-learning algorithms Integrates with SageMaker Clarify to show how models come to predictions Shows feature importance for a given prediction Suitable problems: classification, regression, PCA , some missing values Not for autopilot: vision/text/sequence based data, mostly missing data, features need interpretation, need a pretrained models","title":"SageMaker Autopilot"},{"location":"AWS/SageMaker/SageMaker%20Canvas/","text":"No-code machine learning Can join datasets, perform automatic data cleaning, and perform classification and regression","title":"SageMaker Canvas"},{"location":"AWS/SageMaker/SageMaker%20Clarify/","text":"","title":"SageMaker Clarify"},{"location":"AWS/SageMaker/SageMaker%20Debugger/","text":"Saves internal model state (gradients/tensors) at periodic intervals Define rules to detect unwanted conditions (e.g. exploding/vanishing gradients) in training Logs a cloudwatch event when unwanted conditions are encounters Integrates with SageMaker Studio via SageMaker Studio Debugger Autogenerates training reports Several built in rules: Monitor system bottlenecks Profile model framework (e.g. tensorflow) operations Debug model parameters Supports Tensorflow, PyTorch, MXNet, XGBoost, SageMaker generic estimator Can create custom hooks and rules","title":"SageMaker Debugger"},{"location":"AWS/SageMaker/SageMaker%20Edge%20Manager/","text":"Software agent for edge devices Model optimized for edge devices with SageMaker Neo Can collect and sample data for monitoring, labeling, and retraining","title":"SageMaker Edge Manager"},{"location":"AWS/SageMaker/SageMaker%20JumpStart/","text":"Import/transform/analyze/export data within SageMaker Studio","title":"SageMaker JumpStart"},{"location":"AWS/SageMaker/SageMaker%20Model%20Monitor/","text":"Get alerts on quality deviations on deployed models via Cloudwatch Can visualize data drift (how incoming data is drifting from training data) Detects anomalies and outliers Integrates with SageMaker Clarify to monitor for bias automatically Can also monitor feature importance drift","title":"SageMaker Model Monitor"},{"location":"AWS/SageMaker/SageMaker%20Neo/","text":"","title":"SageMaker Neo"},{"location":"AWS/SageMaker/SageMaker%20Notebooks/","text":"alias: [notebook] \u00b6","title":"SageMaker Notebooks"},{"location":"AWS/SageMaker/SageMaker%20Notebooks/#alias-notebook","text":"","title":"alias: [notebook]"},{"location":"AWS/SageMaker/SageMaker%20Studio/","text":"","title":"SageMaker Studio"},{"location":"AWS/SageMaker/SageMaker%20Training%20Compiler/","text":"Optimizes model training by converting them into hardware-optimized instructions Can accelerate training by up to 50%","title":"SageMaker Training Compiler"},{"location":"AWS/SageMaker/SageMaker/","text":"","title":"SageMaker"},{"location":"AWS/SageMaker/Sagemaker%20Ground%20Truth/","text":"Service for a human to provide ground truth to an unlabeled dataset Contains its own ML model so that it can learn over time and only send uncertain data to people","title":"Sagemaker Ground Truth"},{"location":"AWS/SageMaker/Semantic%20Segmentation/","text":"Pixel-level object classification: each pixel is given an object label Output is a segmentation mask Input can be jpg or png with annotations, or augmented manifest image format, which enables support for pipe mode jpg accepted for Inference Uses MXNet Gluon and Gluon CV with supports for 3 algorithms: Fully-Convolutional network (FCN) Pyramid scene parsing (PSP) DeepLabV3 Two backbones are supported: ResNet50 and ResNet101, both optionally pre-trained on imageNet for transfer learning Hyperparameters are the usual, but algorithm and backbone can be chosen Only GPU on a single machine supported for training","title":"Semantic Segmentation"},{"location":"AWS/SageMaker/Seq2Seq/","tags":["NLP"],"text":"Takes a sequence of tokens, outputs a sequence of tokens Use cases: Machine translation Text summarization Speech to text Implemented with RNN 's and CNN 's with attention Input format is RecordIO-Protobuf with tokens as integers Uses a vocabulary file to map words to number SageMaker provides pre-trained models for common tasks e.g. machine translation Hyperparameters batch_size optimizer_type num_layers_encoder num_layers_decoder learning_rate Metrics Accuracy - not very well suited for this task BLEU score - compares against multiple reference translations Perplexity - cross-entropy based metric Trains on single instance GPU nodes, but can use multiple GPUs in a single machine","title":"Seq2Seq"},{"location":"AWS/SageMaker/XGBoost/","tags":["Classification","Regression"],"text":"eXtreme Gradient boosted decision trees Powerful and quick to run Performs classification and regression Takes CSV, libsvm, recordIO-protobuf, and parquet formats Many hyperparameters to tune Subsample - prevents overfitting Eta - step size shrinkage; prevents overfitting Gamma - minimum loss reduction to partition tree; larger value prevents overfitting Alpha - L1 regularization term Lambda - L2 regularization term eval_metric - metric to optimize on e.g. AUC, error, RMSE scale_pos_weight - adjusts balance of weights; useful for unbalanced classes max_depth - max depth of tree; setting too high can overfit Training uses CPU's only for multiple instance training Memory bound algorithm With newer XGBoost versions, single-instance GPU training is available In general, training on GPU can be more cost-efficient due to faster training","title":"XGBoost"},{"location":"AWS/SageMaker/High%20Level%20Services/CodeGuru/","text":"Automated code review for java and python code - analyzes source code and finds performance problems Identifies resource leaks and race conditions Machine learning powered","title":"CodeGuru"},{"location":"AWS/SageMaker/High%20Level%20Services/Comprehend/","tags":["NLP"],"text":"Natural Language Processing and Text Analytics Inputs: social media, emails, web pages, documents, transcripts Can input medical records with comprehend medical Extracts key phrases, entities, sentiment, language, syntax, topics, and document classifications Can train on own data","title":"Comprehend"},{"location":"AWS/SageMaker/High%20Level%20Services/Connect/","text":"Call center service Finds phrases/service agents that correlate with successful/unsuccessful calls Identifies themes among customer questions","title":"Connect"},{"location":"AWS/SageMaker/High%20Level%20Services/DeepLens/","text":"Deep learning enabled video camera for integration with Rekognition , Sagemaker , Polly , and other ML frameworks Meant for developers training models, not production / security camera scenarios","title":"DeepLens"},{"location":"AWS/SageMaker/High%20Level%20Services/Forecast/","tags":["Forecasting"],"text":"Fully managed time series forecasting Chooses best model for data e.g. ARIMA, DeepAR , ETS, NPTS, Prophet Can combine multiple time series to find associations between time series","title":"Forecast"},{"location":"AWS/SageMaker/High%20Level%20Services/Fraud%20Detector/","text":"User uploads historical fraud data, builds custom model and exposes and API for online applications","title":"Fraud Detector"},{"location":"AWS/SageMaker/High%20Level%20Services/Kendra/","text":"Natural language search engine Tailored for building search engine for enterprise intranets End users provide feedback with thumbs up/down to improve machine learning results","title":"Kendra"},{"location":"AWS/SageMaker/High%20Level%20Services/Lex/","text":"\"Inner workings of Alexa\" Natural-language chatbot engine Intents are defined Customer utterances trigger events Slots specify extra information needed by the intent Example: Customer says \"I want to order a pizza\", this triggers an intent, customer specifies pizza size, toppings, etc which provide data to slots in the intent Can deploy to AWS Mobile SDK, Meta Messenger, Slack, Twilio Can generate chatbots automatically from existing conversation transcripts Integrates with Amazon Connect to provide transcripts","title":"Lex"},{"location":"AWS/SageMaker/High%20Level%20Services/Lookout/","text":"Monitors industrial equipment for anomalies and metrics, can use computer vision to detect defects in products","title":"Lookout"},{"location":"AWS/SageMaker/High%20Level%20Services/Monitron/","text":"Physical sensor with associated service and phone app to capture data on industrial machines, end-to-end system","title":"Monitron"},{"location":"AWS/SageMaker/High%20Level%20Services/Neuron/","text":"SDK for Amazon's custom ML inference chips","title":"Neuron"},{"location":"AWS/SageMaker/High%20Level%20Services/Panorama/","text":"Computer vision for edge devices - gives computer vision to existing IP cameras","title":"Panorama"},{"location":"AWS/SageMaker/High%20Level%20Services/Personalize/","text":"Fully managed recommender engine - same one used by Amazon Fed in data from past purchases, ratings, impressions, cart adds, catalog, user demographics Explicted schema provided in Avro format Data can be provided with S3 or in real time via API calls Will reccomend popular items for users with no data (new users) Can provide contextual reccomendations based on device type or time Hyperparameters: hidden_dimension - automaticall optimized back-propogation through time (bptt) - enabling gives higher weight to recent events via RNN recency_mask - weights recent events more min/max_user_history_length_percentile - filters out robots/webcrawlers exploration_weight - controls relevance of items to uesrs exploration_item_age_cut_off - controls how far to look back for reccomendations Security for user data (stored in S3 ) access is controlled via IAM","title":"Personalize"},{"location":"AWS/SageMaker/High%20Level%20Services/Polly/","text":"Text to speech Can be given plain text, Speech Synthesis Markup Languge (SSML), or Speech Marks SSML gives control over emphasis, pronunciation, breathing, whispering, speech rate, pitch, and pauses Speech marks can encode when sentence/word starts/ends in the output Can be given custom lexicons to show the model how to pronounce specific words and phrases Provides multiple language and voice options","title":"Polly"},{"location":"AWS/SageMaker/High%20Level%20Services/Rekognition/","tags":["ComputerVision","IncrementalTraining"],"text":"Computer vision Service Can do: Object and scene detection (can provide custom face collection) Image moderation Facial Analaysis (emotions and things like detecting glasses) Celebrity recognition Face comparison Find text in images Video analysis such as detecting objects/people/celebrities in video and finding paths of people Images come from S3 Video must come from Kinesis Video Streams Can be provided with custom data e.g. company logos","title":"Rekognition"},{"location":"AWS/SageMaker/High%20Level%20Services/TexTract/","tags":["ComputerVision"],"text":"OCR with forms, fields, tables support","title":"TexTract"},{"location":"AWS/SageMaker/High%20Level%20Services/TorchServe/","text":"Service to deploy PyTorch Models","title":"TorchServe"},{"location":"AWS/SageMaker/High%20Level%20Services/Transcribe/","text":"Speech to text Can perform speaker indentification, channel identification (seperate transcripts for each person speaking), automatic language indentification, and custom vocabularies Can use automatic content redaction to redact PII Can supply custom offensive words list for censorship purposes","title":"Transcribe"},{"location":"AWS/SageMaker/High%20Level%20Services/Translate/","tags":["NLP"],"text":"Deep learning based translation service Supports custom terminology in CSV or TMX format for things such as brand names or proper nouns","title":"Translate"},{"location":"Data%20Analysis%20and%20Preparation/Binning/","text":"Binning \u00b6 Technique to transform numerical data into categorical data e.g. continuous age data can be binned into 20-29, 30-39, etc. Useful when there is uncertainty in measurements as that scenario does not neccessarily lose information by binning Quantile binning can be used to give equal sized bins","title":"Binning"},{"location":"Data%20Analysis%20and%20Preparation/Binning/#binning","text":"Technique to transform numerical data into categorical data e.g. continuous age data can be binned into 20-29, 30-39, etc. Useful when there is uncertainty in measurements as that scenario does not neccessarily lose information by binning Quantile binning can be used to give equal sized bins","title":"Binning"},{"location":"Data%20Analysis%20and%20Preparation/Data%20Categories/","text":"Numerical - quantitative data e.g. heights or lengths. Can be discrete or continuous Categorical - qualitative data with no inherent numericla meaning e.g. gender or product type. Frequently one-hot encoded for ML purposes Ordinal - categorical data with numerical meaning e.g. movie ratings on a 1-5 scale or numerical binned into categories","title":"Data Categories"},{"location":"Data%20Analysis%20and%20Preparation/Distribution%20Types/","text":"Gaussian - continuous bell curve, parameterized via mean (mu) and std. dev. (sigma). Support = [-inf, inf] Poisson - discrete, expresses the probability of an event with a known mean time between events occuring Bernoulli - discrete, gives the probability of outcome with known probability p Binomial - discrete, generalization of Bernoulli, gives the probability of independent repeated Bernoulli trials","title":"Distribution Types"},{"location":"Data%20Analysis%20and%20Preparation/Imbalanced%20Data/","text":"Oversampling - simply duplicate samples from minority class. Works well for Neural Networks Undersampling - drop data from majority class. Not ideal. SMOTE - Synthetic Minority Oversampling TEchnique - oversamples from minority class using KNN and undersamples majority clas Probablity threshold can be changed to adjust tradeoff of false positives/false negatives - Confusion Matrix#ROC Curve shows the tradeoff","title":"Imbalanced Data"},{"location":"Data%20Analysis%20and%20Preparation/Missing%20Data%20Techniques/","text":"Mean replacement \u00b6 Fast and easy Works well when outliers are not present Major drawback: does not take into account correlations between columns Median replacement \u00b6 Similar to mean replacement but works better when outliers are present Dropping data \u00b6 Generally bad choice Can bias dataset (potential reason why data is missing) Replacing data with other columns \u00b6 Situational e.g. a dataset with missing full text reviews can have missing values filled with the review summary Machine learning \u00b6 K Nearest Neighbors ( KNN ) can be used to find similar rows and average their values - good for numerical data Deep learning models work well for filling in missing categorical data Regression techniques can be used for numerical data (MICE is a good algorithm for missing data)","title":"Mean replacement"},{"location":"Data%20Analysis%20and%20Preparation/Missing%20Data%20Techniques/#mean-replacement","text":"Fast and easy Works well when outliers are not present Major drawback: does not take into account correlations between columns","title":"Mean replacement"},{"location":"Data%20Analysis%20and%20Preparation/Missing%20Data%20Techniques/#median-replacement","text":"Similar to mean replacement but works better when outliers are present","title":"Median replacement"},{"location":"Data%20Analysis%20and%20Preparation/Missing%20Data%20Techniques/#dropping-data","text":"Generally bad choice Can bias dataset (potential reason why data is missing)","title":"Dropping data"},{"location":"Data%20Analysis%20and%20Preparation/Missing%20Data%20Techniques/#replacing-data-with-other-columns","text":"Situational e.g. a dataset with missing full text reviews can have missing values filled with the review summary","title":"Replacing data with other columns"},{"location":"Data%20Analysis%20and%20Preparation/Missing%20Data%20Techniques/#machine-learning","text":"K Nearest Neighbors ( KNN ) can be used to find similar rows and average their values - good for numerical data Deep learning models work well for filling in missing categorical data Regression techniques can be used for numerical data (MICE is a good algorithm for missing data)","title":"Machine learning"},{"location":"Data%20Analysis%20and%20Preparation/Outliers%20In%20Dataset/","text":"Defining outliers \u00b6 No single process - can be done via variance, AWS Random Cut Forest , or via a multiple of IQR Dropping outliers is situational","title":"Defining outliers"},{"location":"Data%20Analysis%20and%20Preparation/Outliers%20In%20Dataset/#defining-outliers","text":"No single process - can be done via variance, AWS Random Cut Forest , or via a multiple of IQR Dropping outliers is situational","title":"Defining outliers"},{"location":"Data%20Analysis%20and%20Preparation/Probability%20Density%20Function/","text":"Gives probability of data falling within a range With continuous data, the probability is the integral of the PDF over a range With discrete data, the probability is the sum of the PMF over a range","title":"Probability Density Function"},{"location":"Data%20Analysis%20and%20Preparation/SQL/","text":"","title":"SQL"},{"location":"Data%20Analysis%20and%20Preparation/TF-IDF/","text":"Text frequency-inverse document frequency Example: Document 1 (d1) : \u201cA quick brown fox jumps over the lazy dog. What a fox!\u201d Document 2 (d2) : \u201cA quick brown fox jumps over the lazy fox. What a fox!\u201d Question: which document is the word \"fox\" more relevant to, using tf-idf tf(\u201cfox\u201d, d1) = 2/12 , as the word \"fox\" appears twice in the first document which has a total of 12 words tf(\u201cfox\u201d, d2) = 3/12 , as the word \"fox\" appears thrice in the second document which has a total of 12 words An idf is constant per corpus (in this case, the corpus consists of 2 documents), and accounts for the ratio of documents that include that specific \"term\". Using this definition, we can compute the following: idf(\u201cfox\u201d, D) = log(2/2) = 0 , as the word \"fox\" appears in both the documents in the corpus tf-idf(\u201cfox\u201d, d1, D) = tf(\u201cfox\u201d, d1) * idf(\u201cfox\u201d, D) = (2/12) * 0 = 0 tf-idf(\u201cfox\u201d, d2, D) = tf(\u201cfox\u201d, d2) * idf(\u201cfox\u201d, D) = (3/12) * 0 = 0 Using tf-idf, the word \u201cfox\u201d is equally relevant (or just irrelevant!) for both document d1 and document d2","title":"TF IDF"},{"location":"Data%20Analysis%20and%20Preparation/Time%20Series/","text":"Time series can be broken down into trend, seasonality, noise Trend governs how the mean of the time series changes over time Seasonality governs how the time series fluctuates around the mean over time If seasonal variance is constant, time series can be represented as trend + seasonality + noise If seasonal variance is promotional to the trend, series can be represented as trend * seasonality * noise","title":"Time Series"},{"location":"Data%20Analysis%20and%20Preparation/Transformations/","text":"Transformations \u00b6 Applying functions to data may make models perform better on it For example, the logarithm of exponentially trending data will give a linear feature Categorical data which is originally represented with numbers should be one-hot encoded with some models e.g. neural networks Scaling to transform features to a certain mean and range is required with some models such as regressions This can be done with SciKitLearn 's standard scalar (scales to 0 mean/unit variance) or MinMaxScalar (scales to given range) Input data should always be shuffled","title":"Transformations"},{"location":"Data%20Analysis%20and%20Preparation/Transformations/#transformations","text":"Applying functions to data may make models perform better on it For example, the logarithm of exponentially trending data will give a linear feature Categorical data which is originally represented with numbers should be one-hot encoded with some models e.g. neural networks Scaling to transform features to a certain mean and range is required with some models such as regressions This can be done with SciKitLearn 's standard scalar (scales to 0 mean/unit variance) or MinMaxScalar (scales to given range) Input data should always be shuffled","title":"Transformations"},{"location":"Data%20Analysis%20and%20Preparation/Visualization%20Types/","text":"Bar charts - comparison and distribution Line graphs - changes over a given ordered domain e.g. time Scatter plots - for seeing correlations/outliers, shows individual data points Heat maps - for seeing correlations, does not show individual data points Pie graphs, tree maps - for aggregating data","title":"Visualization Types"},{"location":"Machine%20Learning/Activation%20Functions/","text":"Neural networks need activation functions between neurons to add non-linearity. This is fundamental to learning - otherwise a neural network's output could be represented as a linear combination of its inputs In general, activation functions work well when they are quick to compute, continuous, and differentiable Sigmoid - logistic curve from 0 to 1. Used as the output layer on binary classification problems as it outputs a confidence. Can also be used for multiclass, multilabel classification e.g. detecting multiple objects in a single image TanH - curve from -1 to 1 Both of these are slow to compute and have small gradients at high/low input values which can lead to \"vanishing gradients\" and stop the network from leanring ReLU - good general purpose choice. Defined as 0 for x<=0 and x for x>0. Fast to compute, but can lead to \"dying ReLUs\" when input is not greater than 0. Leaky ReLU - solves dying ReLUs but having the function be a*x for x<=0, a is user defined Parametric ReLU - same as leaky ReLU but a is defined via backpropogation, however this doesn't help in many cases and is computationally expensive ELU - exponential linear unit. Like ReLU but smooth and continuously differentiable Softmax - like sigmoid but with multiple inputs. Often used as the output layer of a multiclass classification problem. Converts output of neural network to confidences for each class.","title":"Activation Functions"},{"location":"Machine%20Learning/Confusion%20Matrix/","text":"Actual Yes Actual No Predicted Yes True Positive (TP) False Positive (FP) Predicted No False Negative (FN) True Negative (TN) Recall - AKA sensitivity, true positive rate, completeness. Pecent of positives predicted correctly. Good choice of metric where false negatives are important e.g. fraud detection (missing a case of fraud is worse than accidently marking a legitimate transaction as fraud) Formula: TP/(TP+FN) Precision - AKA correct positives. Percent of relevant results. Good choice when false positives are more important e.g. drug testing Formula: TP/(TP+FP) Specificity - AKA true negative rate Formula: TN/(TN+FP) F1 Score - harmonic mean of precision and recall Formula: (2 TP) / (2 TP + FP + FN) = ( 2 * precision * recall ) / ( precision * recall ) Accuracy - fraction of correct predictions. Can be very misleading especially in inbalanced classification problems Formula: (TP + TN) / (TP + TN + FP + FN) ROC Curve \u00b6 ROC Curve - Receiver operating characteristic curve. Plot of recall (true positive rate) vs false positive rate at various thresholds Diagonal line represents random guessing AUC is a good metric of classifier's ability - equal to probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one","title":"Confusion Matrix"},{"location":"Machine%20Learning/Confusion%20Matrix/#roc-curve","text":"ROC Curve - Receiver operating characteristic curve. Plot of recall (true positive rate) vs false positive rate at various thresholds Diagonal line represents random guessing AUC is a good metric of classifier's ability - equal to probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one","title":"ROC Curve"},{"location":"Machine%20Learning/Convolutional%20Neural%20Networks/","text":"Used to extract patterns from images Generally useful for when the feature's location in the input data is unknown Uses convolution to take features from other layers and eventually recognize more complex features Data augmentation is often very useful to prevent overfitting","title":"Convolutional Neural Networks"},{"location":"Machine%20Learning/Debugging/","text":"Not Learning \u00b6 Possible causes: \u00b6 Not enough training data Data not normalized for algos that require its Data not shuffled Heavily Imbalanced Data Too many Outliers In Dataset Data leakage/bad data pipeline Use SageMaker Debugger to detect dying ReLU/exploding/vanishing gradients - switch activation func-","title":"Not Learning"},{"location":"Machine%20Learning/Debugging/#not-learning","text":"","title":"Not Learning"},{"location":"Machine%20Learning/Debugging/#possible-causes","text":"Not enough training data Data not normalized for algos that require its Data not shuffled Heavily Imbalanced Data Too many Outliers In Dataset Data leakage/bad data pipeline Use SageMaker Debugger to detect dying ReLU/exploding/vanishing gradients - switch activation func-","title":"Possible causes:"},{"location":"Machine%20Learning/Ensemble%20Methods/","text":"A common example of an ensemble method is a random forest - it takes many decision trees and lets them vote in parallel. Ensemble methods reduce overfitting and improve the predictiveness of a model Bagging generates random training sets by random sampling with replacement, and trains models in parallel with these datasets, and these models vote on the result Boosting trains models serially, and with each iteration adjusts the weights of the observations so that the model becomes more predictive Boosting generally is more accurate, but bagging helps avoid overfitting and bagging trains faster XGBoost is a popular boosting framework","title":"Ensemble Methods"},{"location":"Machine%20Learning/Neural%20Network%20Tuning/","text":"Learning rate - governs how fast gradient descent happens. Low learning rates can cause slow training, while overly high learning rates can cause the network to miss an optimal solution. Learning rate can be decreased over time to decrease training time and give good results Batch size governs how many samples are passed to the network at once. Smaller batch sizes can train better since they can help the network escape local minima in the loss surface better when performing gradient descent. Larger batch sizes can decrease training time by leveraging multiprocessing on the GPU and provide normalizing effects.","title":"Neural Network Tuning"},{"location":"Machine%20Learning/Overfitting/","text":"\u2022 How to detect: training loss keeps decreasing, validation loss stays the same or goes up \u2022 Causes: Model structure too complicated/large for data \u2022 Increase regularization with - Dropout - Batch Normalization - Decreasing model size - network width/depth for NN - L1/L2 regularization - Early stopping - More training data \u2022 Can adjust model specific hyperparameters tuning: - max_depth and min_samples_leaf for random forest/decision tree - K for KNN - Regularization and Eta/Subsample parameters for XGBoost","title":"Overfitting"},{"location":"Machine%20Learning/Recurrent%20Neural%20Networks/","text":"For sequential data, such as language or time series data Can do things like predict stock prices from historical data, sentiment analysis, or perform language translation Transformers have longer memories and work better on longer inputs - generally transformers have surpassed RNNs in most tasks recently. LSTM \u00b6 Long Short-Term Memory A type of RNN that is more powerful than standard RNN","title":"Recurrent Neural Networks"},{"location":"Machine%20Learning/Recurrent%20Neural%20Networks/#lstm","text":"Long Short-Term Memory A type of RNN that is more powerful than standard RNN","title":"LSTM"},{"location":"Machine%20Learning/Regularization/","text":"Regularization is useful to prevent overfitting, which is extremely easy to do in deep learning Too many layers and neurons can cause overfitting - decreasing the model's complexity can prevent overfitting by not letting the model learn the noise in the training data Dropout drops neurons at random each time data is passed through the network to prevent individual neurons from overfitting to data Early stopping can be used to decrease training time by stopping when the network stops learning on the validation set. This can cut the training off before the network overtrains. L1 regularization adds the sum of the weights in the network to the weights. This can cause the effect of entire features to go to 0. This lets the network select only the important features. This technique is inefficient. L2 regularization is a similar concept but considers the square of the weights. This ends up weighting less important features less instead of dropping them, and is quicker to compute.","title":"Regularization"},{"location":"Machine%20Learning/SciKitLearn/","text":"","title":"SciKitLearn"}]}